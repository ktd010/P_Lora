{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification\n",
    "from minlora import add_lora, get_lora_params\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained Vision Transformer model directly to the device\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\", \n",
    "    attn_implementation=\"sdpa\", \n",
    "    torch_dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "# Adjust classifier for 100 classes\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 100).to(device)\n",
    "\n",
    "# Add LoRA layers to the model\n",
    "add_lora(model)\n",
    "\n",
    "# Freeze all parameters except LoRA\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Enable only LoRA parameters for training\n",
    "for param in get_lora_params(model):\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Save model setup for next notebook\n",
    "torch.save(model.state_dict(), \"vit_lora_initialized.pth\")\n",
    "print(\"Model setup complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                                                           Output Shape              Param #\n",
       "==================================================================================================================================\n",
       "ViTForImageClassification                                                        [1, 100]                  --\n",
       "├─ViTModel: 1-1                                                                  [1, 197, 768]             --\n",
       "│    └─ViTEmbeddings: 2-1                                                        [1, 197, 768]             152,064\n",
       "│    │    └─ViTPatchEmbeddings: 3-1                                              [1, 196, 768]             (590,592)\n",
       "│    │    └─Dropout: 3-2                                                         [1, 197, 768]             --\n",
       "│    └─ViTEncoder: 2-2                                                           [1, 197, 768]             --\n",
       "│    │    └─ModuleList: 3-3                                                      --                        85,718,016\n",
       "│    └─LayerNorm: 2-3                                                            [1, 197, 768]             (1,536)\n",
       "├─ParametrizedLinear: 1-2                                                        [1, 100]                  100\n",
       "│    └─ModuleDict: 2-4                                                           --                        --\n",
       "│    │    └─ParametrizationList: 3-4                                             [100, 768]                80,272\n",
       "==================================================================================================================================\n",
       "Total params: 86,542,580\n",
       "Trainable params: 667,024\n",
       "Non-trainable params: 85,875,556\n",
       "Total mult-adds (M): 115.79\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 711.56\n",
       "Params size (MB): 5.18\n",
       "Estimated Total Size (MB): 717.34\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Dataset setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset_path = '/mnt/c/Users/kdtar/Kasun_stuff/My_datasets/mini_imgenet'\n",
    "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Split dataset into training and validation subsets\n",
    "total_size = len(dataset)\n",
    "subset_size = int(0.1 * total_size)  # Use 10% of the total dataset\n",
    "train_size = int(0.8 * subset_size)\n",
    "val_size = subset_size - train_size\n",
    "\n",
    "subset_dataset, _ = random_split(dataset, [subset_size, total_size - subset_size])\n",
    "train_dataset, val_dataset = random_split(subset_dataset, [train_size, val_size])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False, num_workers=4)\n",
    "\n",
    "# Save Dataloaders\n",
    "torch.save((train_loader, val_loader), \"dataloaders.pth\")\n",
    "print(\"Dataloaders prepared and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from minlora import get_lora_params\n",
    "\n",
    "# Load model and data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import ViTForImageClassification\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model.load_state_dict(torch.load(\"vit_lora_initialized.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "train_loader, val_loader = torch.load(\"dataloaders.pth\")\n",
    "\n",
    "# Loss and optimizer setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(get_lora_params(model), lr=1e-3)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / total, 100 * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / total, 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from notebook3 import train_epoch, validate\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Model & Data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"vit_lora_initialized.pth\"))\n",
    "model.to(device)\n",
    "train_loader, val_loader = torch.load(\"dataloaders.pth\")\n",
    "\n",
    "# Optimizer, Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(get_lora_params(model), lr=1e-3)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
